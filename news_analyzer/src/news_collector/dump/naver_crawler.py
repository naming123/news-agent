# naver_crawler.py
from __future__ import annotations
import os
import random, time
from typing import List, Dict
import itertools
import requests
from bs4 import BeautifulSoup

class NaverNewsCrawler:
    """
    Naver News crawler with HTML parsing research
    """
    def __init__(self):
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148"
        ]
        self._ua_cycle = itertools.cycle(self.user_agents)
        self.session_headers = {"User-Agent": next(self._ua_cycle)}

    def random_delay(self, min_sec: int = 1, max_sec: int = 5) -> None:
        time.sleep(random.uniform(min_sec, max_sec))

    def rotate_user_agent(self) -> None:
        self.session_headers["User-Agent"] = next(self._ua_cycle)

    def search_news_mock(
        self, keyword: str, start_date: str, end_date: str, num_items: int = 3
    ) -> List[Dict]:
        self.rotate_user_agent()
        self.random_delay(1, 2)
        samples = []
        medias = ["ì—°í•©ë‰´ìŠ¤", "ì¡°ì„ ë¹„ì¦ˆ", "ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ", "ì„œìš¸ê²½ì œ"]
        headlines_pos = [
            f"{keyword} ì‹¤ì  í˜¸ì¡°, ì„±ì¥ ê¸°ëŒ€",
            f"{keyword} ì£¼ê°€ ìƒìŠ¹",
            f"{keyword} í˜ì‹  ì‹ ê¸°ë¡",
        ]
        headlines_neg = [
            f"{keyword} ë¦¬ì½œ ì´ìŠˆ, ì‹¤ì  ë¶€ë‹´",
            f"{keyword} ì£¼ê°€ í•˜ë½",
            f"{keyword} ì•…ì¬ ë…¸ì¶œ",
        ]
        headlines_neu = [
            f"{keyword} ë³´ë„ìë£Œ ë°œí‘œ",
            f"{keyword} ì‹ ê·œ ì‚¬ì—… ê²€í† ",
            f"{keyword} ì—…ê³„ ë™í–¥",
        ]
        pool = headlines_pos + headlines_neg + headlines_neu
        for i in range(num_items):
            title = random.choice(pool)
            summary = f"{keyword} ê´€ë ¨ ìš”ì•½ ê¸°ì‚¬ ë‚´ìš©ì…ë‹ˆë‹¤. ë¬´ì‘ìœ„ ìƒ˜í”Œ {i+1}."
            samples.append({
                "title": title,
                "link": f"https://news.naver.com/mock/{keyword}/{i+1}",
                "media": random.choice(medias),
                "date": random.choice([start_date, end_date]),
                "summary": summary,
                "keyword": keyword,
                "source": "mock"
            })
        return samples

    def search_news_html(self, keyword: str, start: int = 1) -> List[Dict]:
        url = (
            f"https://search.naver.com/search.naver"
            f"?where=news&ie=utf8&sm=nws_hty&query={keyword}&start={start}"
        )
        
        print(f"ğŸ” Original keyword: {keyword}")
        print(f"ğŸ“¡ Request URL: {url}")
        
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            )
        }
        res = requests.get(url, headers=headers)
        
        print(f"âœ… Response status: {res.status_code}")
        print(f"ğŸ“„ HTML length: {len(res.text)} chars")
        
        soup = BeautifulSoup(res.text, "html.parser")
        
        # HTML êµ¬ì¡° ë””ë²„ê¹… - ì‹¤ì œ ë„¤ì´ë²„ ë‰´ìŠ¤ ì…€ë ‰í„°ë“¤
        selectors_to_try = [
            "a.news_tit",                           # ê¸°ì¡´ ë°©ì‹
            ".list_news a[href*='news.naver.com']", # ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬
            ".api_subject_bx a",                    # API ê²°ê³¼ ì˜ì—­ì˜ ë§í¬
            ".news_wrap a",                         # ë‰´ìŠ¤ ë˜í•‘ ì˜ì—­
            ".group_news a",                        # ë‰´ìŠ¤ ê·¸ë£¹ ì˜ì—­
            "div[data-module] a[href*='news.naver.com']", # ë°ì´í„° ëª¨ë“ˆ ë‚´ ë‰´ìŠ¤ ë§í¬
            ".news_area a",                         # ë‰´ìŠ¤ ì˜ì—­
            "a[href*='news.naver.com'][title]"      # title ì†ì„±ì„ ê°€ì§„ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬
        ]
        
        items = []
        print("\nğŸ” ì…€ë ‰í„°ë³„ ê²€ìƒ‰ ê²°ê³¼:")
        
        for selector in selectors_to_try:
            try:
                elements = soup.select(selector)
                print(f"  {selector}: {len(elements)}ê°œ ë°œê²¬")
                
                if elements and not items:  # ì²« ë²ˆì§¸ë¡œ ë°œê²¬ëœ ì…€ë ‰í„° ì‚¬ìš©
                    print(f"âœ… '{selector}' ì‚¬ìš©í•˜ì—¬ íŒŒì‹± ì‹œì‘")
                    
                    for i, a_tag in enumerate(elements[:10]):  # ìµœëŒ€ 10ê°œ
                        title = a_tag.get("title") or a_tag.get_text(strip=True)
                        link = a_tag.get("href")
                        
                        # ë‰´ìŠ¤ ì œëª© ê¸¸ì´ ë° ìœ íš¨ì„± ì²´í¬
                        if title and link and len(title) > 5:
                            print(f"    [{i+1}] {title[:50]}...")
                            items.append({
                                "title": title,
                                "link": link,
                                "keyword": keyword,
                                "source": "html",
                                "selector": selector
                            })
            except Exception as e:
                print(f"  âŒ {selector}: ì—ëŸ¬ {e}")
        
        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ëª¨ë“  ë§í¬ì—ì„œ ë‰´ìŠ¤ ê´€ë ¨ ê²ƒë§Œ ì¶”ì¶œ
        if not items:
            print("\nğŸ” ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ëª¨ë“  ë§í¬ ë¶„ì„")
            all_links = soup.find_all("a", href=True)
            news_links = [
                a for a in all_links 
                if "news.naver.com" in a.get("href", "")
                or any(cls in (a.get("class", []) or []) for cls in ["news", "tit", "title"])
            ]
            
            print(f"  ì „ì²´ ë§í¬: {len(all_links)}ê°œ")
            print(f"  ë‰´ìŠ¤ ê´€ë ¨ ë§í¬: {len(news_links)}ê°œ")
            
            for i, a_tag in enumerate(news_links[:10]):
                title = a_tag.get("title") or a_tag.get_text(strip=True)
                link = a_tag.get("href")
                
                if title and len(title) > 5:
                    print(f"    [{i+1}] {title[:50]}...")
                    items.append({
                        "title": title,
                        "link": link,
                        "keyword": keyword,
                        "source": "html",
                        "selector": "fallback_search"
                    })
        
        print(f"\nğŸ“° ì´ {len(items)}ê°œ ë‰´ìŠ¤ ì•„ì´í…œ ìˆ˜ì§‘ ì™„ë£Œ")
        return items
    def search_news_html_multi_page(self, keyword: str, max_pages: int = 3) -> List[Dict]:
        """
        ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ìŠ¤í¬ë¡¤ íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜)
        """
        all_items = []
        
        print(f"ğŸ” {keyword} - {max_pages}í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘")
        
        for page in range(1, max_pages + 1):
            start = (page - 1) * 10 + 1  # ë„¤ì´ë²„ëŠ” 10ê°œì”© í˜ì´ì§•
            
            print(f"\nğŸ“„ í˜ì´ì§€ {page}/{max_pages} (start={start})")
            print("-" * 50)
            
            try:
                # ê° í˜ì´ì§€ ìˆ˜ì§‘
                items = self.search_news_html(keyword, start=start)
                
                if items:
                    print(f"âœ… í˜ì´ì§€ {page}: {len(items)}ê°œ ìˆ˜ì§‘")
                    all_items.extend(items)
                else:
                    print(f"âŒ í˜ì´ì§€ {page}: ìˆ˜ì§‘ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
                    break  # ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                
                # í˜ì´ì§€ ê°„ ë”œë ˆì´ (ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€)
                if page < max_pages:
                    print("â±ï¸  ë‹¤ìŒ í˜ì´ì§€ê¹Œì§€ 2ì´ˆ ëŒ€ê¸°...")
                    time.sleep(2)
                    
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±° (ê°™ì€ ë§í¬ëŠ” í•œ ë²ˆë§Œ)
        unique_items = []
        seen_links = set()
        
        for item in all_items:
            if item["link"] not in seen_links:
                unique_items.append(item)
                seen_links.add(item["link"])
        
        print(f"\nğŸ¯ ìˆ˜ì§‘ ì™„ë£Œ:")
        print(f"   - ì´ í˜ì´ì§€: {max_pages}í˜ì´ì§€")
        print(f"   - ì›ë³¸ ìˆ˜ì§‘: {len(all_items)}ê°œ")
        print(f"   - ì¤‘ë³µ ì œê±° í›„: {len(unique_items)}ê°œ")
        
        return unique_items